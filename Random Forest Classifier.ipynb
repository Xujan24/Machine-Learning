{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Random Forest Classifier.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ymm6gvcate2G","colab_type":"text"},"source":["## Bootstrap Aggregation and Random Forest Classifier\n","In the [previous](https://github.com/Xujan24/Machine-Learning/blob/master/Boosting.ipynb) tutorial, we looked at one of the popular ensemble technique in machine learning, boosting, and looked in details on one of the most popular and widely used boosting technique called adaptive boositing, in short `AdaBoost`. In this tutorial, we will be looking another ensemble technique, known as boostrap aggregation, in short `Bagging` and one of it's variant called Random Forest.\n","\n","Like any other ensemble technique, bagging also uses a set of weak learners and convert them to a single, more powerful classifier. Unlike boosting, in which the weak learners are trained in a sequential manner, i.e. one weak learner is trained only after the previous weak learner has been trained, in bagging the weak learners can be trained without any kind of sequential order. This means, every weak learner is trained independentely of other weak learners and doesn't affect the overall performance of the final classifier in whatever sequence they were trained.\n","\n","### What is Bootstrap?\n","In statistics, bootstraping is a process of random sampling from a given sample with replacement. Each of these new samples generated from the given sample is called a bootstrap sample. The key idea about bootstraping is that by resampling a given data and performing inference about the sample using the resampled data, we can actually make an inference about the whole population. Here, *population* means a complete set of elements with a specialized set of characteristics and a *sample* is just a subset of it. For instance, if you are interested in determining the average height of a man, then the population would consist of every man living on this planet and a sample would just be a subset of it. For example, a set of all men in your locality.\n","\n","### Bootstrap Aggregation\n","Bootstrap aggregation, or `bagging` is an ensemble method, where we generate multiple boostrap samples from the given training dataset, train weak classifiers on each of these bootstrap samples and obtain an aggreagte classifier using the trained weak classifiers. When predicting a numerical value, `bagging` aggregates the predictions made by each of the weak classifiers and does a plurality voting when predicting a class. Since it averages the predictions over a collection of boostrap samples, it reduces variance (How? we will discuss in the next section). So, `bagging` can help to achieve better predictive performance, when used with a model that has low biase and high variance. And decision trees are considered to have low biase and high variance, hence decision trees are commmonly used as a weak classifier in `bagging`. However, we can use any kind of learning algorithm as a base learner. A typical bootstrap aggregation algorithm works as follows:\n","1. Generate bootstrap samples ($K$ times) from the given training set: $X=x_1,x_2, ..., x_N$ and $Y = y_1, y_2, ..., y_N$.\n","2. For each generated bootstrap sample, train a base learner $\\varphi(x,L_k)$, where $k = 1,2,..., K$ and $L_k$ represents the $k^{th}$ bootstrap sample.\n","3.After training, make prediction for the unseen samples as below:\n"," 1. For regression, average the predictions made by each of the base learners as the final prediction, i.e. $\\hat{\\varphi} = \\frac{1}{K}\\sum_{k=1}^K \\varphi(x')$.\n"," 2. For classification, predict the final class by simply taking the majority vote. Let $N_j  = \\#\\{k;\\varphi(x') = j\\}$, where $j \\in \\{1,2,..., J\\}$, denotes a class $j$ out of $J$. So, $N_j$ stores how many times the base learners has predicted each of the classes in $J$. Now, the final prediction can be made by taking $argmax_jNj$\n","\n","### Why bagging works?\n","This section describes how aggregating helps to decrease the mean-squared error. The following explanation has been taken from Breiman's work, which you can view through [this](https://www.stat.berkeley.edu/~breiman/bagging.pdf) link.\n","\n","Let $L$ represents a bootstrap sample and each $(y,x)$ case in $B$ are independently drawn from the given probability distribution $P$. Also $y$ is numeric and we predict $y$ by $\\varphi(x,L)$, i.e. $\\varphi(x,L)$ represents our base learner. Suppose, we have $k$ bootstrap samples, then the aggregate predictor will just be an average of $\\varphi(x, L_k)$ over $k$. Let $\\varphi_A$ denotes the aggregate predictor then:<br />\n","<center>$\\varphi_A(x, P) = E_L\\varphi(x, L)$</center>\n","\n","Let $Y, X$ be random variables having the distribution $P$ and independent of $L$. Then the average prediction error $e$ in $\\varphi(x, L)$ will be:\n","<center>$e = E_LE_{Y,X}(Y-\\varphi(X,L))^2$</center>\n","\n","And the error for the aggregate predictior will be:\n","<center>$e_A=E_{Y,X}(Y-\\varphi_A(X, P))^2$</center>\n","\n","\n","Expanding $e$ and using the inequality $EZ^2 \\geq (EZ)^2$, we will get:\n","<center>\n","  $e = EY^2 - 2EYE_L\\varphi(X, L) + E_{Y,X}E_L\\varphi^2(X, L)$ <br />\n","  = $EY^2 - 2EY\\varphi_A + E_{Y,X}\\varphi^2_A$<br />\n","  $\\geq E(Y-\\varphi_A)^2 = e_A$\n","</center>\n","\n","Thus, the aggregate predictor will have lower mean-squared error than that of each of the base predictors. How low is the mean-squared error for the aggregate predictor, compared to each of the base predictors, depends on how unequal the two sides of the inequalities, $E_L \\varphi^2(x, L) \\geq [E_L(\\varphi(x, L)]^2$, are. If $\\varphi(x, L)$ doen't change too much with respect to each of the bootstrap samples, $L$, then the two sides will be nearly equal, and hence aggreation doesn't help. However, if $\\varphi(x, L)$ varies a lot with respect to the bootstrap samples, then the aggregation will produce improvements over base learners.<br />\n","Now, the bagged estimates is not $\\varphi_A(x, P)$ but rather, <br />\n","<center>\n","  $\\varphi_B(x) = \\varphi_A(x, P_L)$<br />\n","</center>\n","\n",", where $P_L$ is called the bootstrap approximation to $P$ and each points, $(y_n, x_n) \\in L$, are distributed with a probability of $\\frac{1}{N}$.<br />\n","\n","Now $\\varphi_B$ is caught into two scenarios: if $P_L$ is unstable then the aggregation will produce improvements over base learners and if it is stable then the aggregation won't improve the performance as $\\varphi_A(x, P)\\simeq\\varphi(x, L)$. Another limitation of bagging is that, for some dataset, if the base learner is close to the limits of accuracy attainable on that dataset, then bagging will not produce further improvement on the base learner. These are also applicable on classification problems. For the similar reasoning behind why bagging works for classification, you can go through the Breiman's work. Alternatively, you can also check [this](http://www.stat.cmu.edu/~ryantibs/datamining/lectures/24-bag.pdf) lecture slides, for a simpler explanation.\n","\n","### Random Forest\n","Random forest is an example of `bagging`. It is a forest of decision trees. And what makes this forest, random, is the fact that while training each tree, it randomly selects a subset of features, from the entire feature space and picks the most favourable feature, from the random subset, to make a split. This process is also referred to as `feature bagging`. The main reason for doing this is to reduce the correlation among the trees. For instance, if one or few input features are very strong predictor of the output then these features will be selected to make a split in multiple trees and makes the trees highly correlated. Hence, by introducing randomness in selecting the feature to make a split would help to reduce this correlation. Tin Kam Ho created the first algorithm for random decision forests by introducing random selection features and later extended by Leo Breiman and Adele Cutler by combining the concept of `bagging` to Ho's idea. The following are some of the desirable characteristics of Random Forest (Breiman L. 2001, *Random Forests*, [pdf](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)):\n","1. Its accuracy is as good as Adaboost and sometimes better.\n","2. It's relatively robust to outliers and noise.\n","3. It's faster than bagging or boosting.\n","4. It gives useful internal estimates of error, strength, correlation and variable importance.\n","5. It's simple and easily parallelized.\n","\n","### Out-of-Bag Error\n","Out-of-bag (OOB) error is a method to measure the prediction error of the bagged estimators. Each of the bootstrap samples contains roughly two-third of the given training samples. This means the remaining one-third of the instances are left out, which serves as a test set without explicitly dividing the given dataset into training and test set. As the forest is built, each tree can thus be tested on the samples that are left out when building that tree. This is as accurate as using a test set of the same size as the training set (Breiman 1996b). <br />\n","\n","Given a training set $T$, from bootstrap samples $T_k$, train a number of decision trees $h(x, T_k)$ and combine their outpus to form the bagged classifier. For each $y, x$ in the training set, aggregate the output of only those trees for which $T_k$ doesnot contains $y, x$. These are called OOB classifiers. Then the OOB estimate for the generalization error is the error rate of the OOB classifiers on the training set. <br />\n","\n","The OOB methods can also be used to estimate the strength and correlation( [pdf](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf), see Appendix II).\n","\n","### Random Forest in Action\n","Now, we have build some theoritical intitutions on bootstrap aggregation and random forest, let's create one using `scikit-learn` package. In this tutorial, we will be using the same wine dataset which was used to in the `AdaBoost` tutorial."]},{"cell_type":"code","metadata":{"id":"GyWSj6UXtdiZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"55d3e2c7-356b-4524-f300-1ce38d3e0e9a","executionInfo":{"status":"ok","timestamp":1558932760769,"user_tz":-600,"elapsed":551,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","\n","wineData = datasets.load_wine()        #returns a bunch object\n","\n","# get the input features X and corresponding classes Y\n","X, Y = wineData.data, wineData.target\n","\n","# get the labels for each class\n","classLabels = wineData.target_names\n","\n","# split the dataset into train and test set.\n","train_x, test_x, train_y, test_y = train_test_split(X, Y,\n","                                                    test_size=0.3,    # 30% will be used as test set\n","                                                    random_state=123, # random seed\n","                                                    shuffle=True      # shuffle the dataset when splitting\n","                                                   )\n","\n","# create a random forest classifier\n","# for detailed documentation visit https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n","model = RandomForestClassifier(n_estimators = 4, # number of trees\n","                               max_depth = 3, # maximum depth of the tree\n","                               min_samples_split = 10, # minimum number of samples required to make a split\n","                               max_features = 4, # number of features to consider when looking for the best split\n","                               random_state = 123\n","                              )\n","\n","# train the model\n","model.fit(train_x, train_y)"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=3, max_features=4, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=10,\n","                       min_weight_fraction_leaf=0.0, n_estimators=4,\n","                       n_jobs=None, oob_score=False, random_state=123,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"6TlQ3nSxcJfu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"0ef00412-5bad-4f65-d0af-275a6ae7d1f2","executionInfo":{"status":"ok","timestamp":1558932764127,"user_tz":-600,"elapsed":750,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["# make prediction on test set\n","\n","pred = model.predict(test_x)\n","print(pred)\n","\n","# calculate the model accuracy\n","acc = model.score(test_x, test_y)\n","print(\"The accuracy of the model is %f on the test dataset.\" %acc)\n"],"execution_count":67,"outputs":[{"output_type":"stream","text":["[2 1 2 1 1 2 0 2 2 1 1 2 2 0 0 2 1 1 0 1 2 2 2 2 1 2 2 1 0 0 0 0 1 1 2 1 2\n"," 0 1 1 2 2 0 0 0 0 0 1 1 1 1 2 2 1]\n","The accuracy of the model is 0.925926 on the test dataset.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vQjIUySGf4iB","colab_type":"text"},"source":["### Using Out-of-Bag Errors to determine the suitable value of `n_estimators`\n","In this part, we will show how the OOB error can be measured at the addition of each new tree during training. This will be helpful to approximate the required number of trees at which the error stabilizes. In this tutorial we will be creating three random forest classifiers with max_features set to 3, 5 and None for which the n_estimators will vary between 1 to 30"]},{"cell_type":"code","metadata":{"id":"edr0ia2DcRFL","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","\n","# set the random seed value\n","RANDOM_STATE = 123\n","\n","clfs = [\n","    (\"RandomForestClassifier, max_features = 3\",\n","    RandomForestClassifier(warm_start = True,\n","                           max_features = 3,\n","                           oob_score = True,\n","                           random_state = RANDOM_STATE)),\n","    (\"RandomForestClassifier, max_features = 5\",\n","    RandomForestClassifier(warm_start = True,\n","                           max_features = 5,\n","                           oob_score = True,\n","                           random_state = RANDOM_STATE)),\n","    (\"RandomForestClassifier, max_features = None\",\n","    RandomForestClassifier(warm_start = True,\n","                           max_features = None,\n","                           oob_score = True,\n","                           random_state = RANDOM_STATE)),\n","]\n","\n","# storing the n_estimators and corresponding OOB_error as\n","# a (<n_estimators>, <error rate>) pairs for each classifiers\n","\n","oob_error_rate = OrderedDict((label, []) for label, _ in clfs)\n","\n","\n","# set the minimum and maximum values for n_estimators\n","min_estimators = 1\n","max_estimators = 30\n","\n","\n","for label, clf in clfs:\n","  for i in range(min_estimators, max_estimators+1):\n","    clf.set_params(n_estimators = i) # set the value for n_estimators\n","    clf.fit(X, Y) # train the model on whole dataset\n","    \n","    oob_error = 1-clf.oob_score_\n","    oob_error_rate[label].append((i, oob_error))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ME6fBZLiiaX-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":284},"outputId":"2764b710-c24b-4636-b72a-ebad551d4c3c","executionInfo":{"status":"ok","timestamp":1558933232225,"user_tz":-600,"elapsed":866,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["# generate the OOB error rate vs n_estimators plot\n","\n","for label, clf_err in oob_error_rate.items():\n","  x, y = zip(*clf_err)\n","  plt.plot(x, y, label = label)\n","\n","  \n","plt.xlim(min_estimators, max_estimators)\n","plt.xlabel(\"n_estimators\")\n","plt.ylabel(\"OOB error rate\")\n","plt.legend(loc = \"upper right\")\n","plt.show()"],"execution_count":99,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4VGX2wPHvm0nvCQmQBglFSjoQ\nELCABaQouKKouIK7P911UWzrquu6spZd68qirF1AXQvgoqiwYkFRQSQIJIROCJBQkpDep7y/PyYZ\nEtImJJN6Ps8zj5m5d+49d4hzcs9973mV1hohhBDibE4dHYAQQojOSRKEEEKIBkmCEEII0SBJEEII\nIRokCUIIIUSDJEEIIYRokCQIIYQQDZIEIYQQokGSIIQQQjTIuaMDaKmgoCAdGRnZ0WEIIUSXsm3b\ntlytdXBL3tPlEkRkZCTJyckdHYYQQnQpSqkjLX2PlJiEEEI0SBKEEEKIBkmCEEII0aAudw2ipzEa\njWRmZlJRUdHRoQghugB3d3fCw8NxcXFp9bYkQXRymZmZ+Pj4EBkZiVKqo8MRQnRiWmtOnz5NZmYm\nUVFRrd6elJg6uYqKCnr16iXJQQjRLKUUvXr1arOKgySILkCSgxDCXm35fdHlEkRh8fGODkEIIXqE\nLpcgsivzOzoEIYToEbpcgjAqqKgo6ugwehSDwUBCQgIxMTFceeWVFBQUtMl2MzIyiImJaZNtzZs3\nj6ioKBISEkhISGDx4sVtst2GfPvtt2zatKnOa2+//TYxMTHExsaSmJjIc889Z4tr1apVbbLf48eP\nM2vWLNvzG264gbi4OF544QX++te/8tVXX7XJftrT3r17SUhIIDExkUOHDrX4/YsWLaKsrMwBkbWN\nTz75hLi4OBISEhg1ahQ//PBDR4fUIl1uFJMGjmT9xJCBkzo6lB7Dw8ODHTt2ADB37lyWLFnCww8/\n3MFR1ffss8/W+QK1l9lsxmAw2L3+t99+i7e3N+PGjQNg3bp1LFq0iPXr1xMaGkplZSVvv/12i+No\nTmhoqC3ZnDx5kq1bt3Lw4MFz2pbJZMLZueP/9//444+ZNWsWf/nLX87p/YsWLeKmm27C09PT7ve0\n57FfeumlXHXVVSilSElJ4brrrmPv3r3tsu+20PG/Iecg/WRyj0wQf/s0jd3H2/bsaXioL49eGW33\n+mPHjiUlJQWAkpISZsyYQX5+PkajkSeeeIIZM2aQkZHBlClTuOCCC9i0aRNhYWF88skneHh4sG3b\nNn7zm98AMGnSmX/DiooKbr/9dpKTk3F2duaf//wnEydOZNmyZXz88ceUlpZy4MAB/vjHP1JVVcU7\n77yDm5sba9euJTAwsNF433//ff7+97+jtWbatGk8/fTTAHh7e/O73/2Or776iiVLluDh4cG9995L\nSUkJQUFBLFu2jJCQEBYvXswrr7yCs7Mzw4cP56mnnuKVV17BYDDw7rvv8uKLL/KPf/yD5557jtDQ\nUADc3Ny49dZb68Xy2GOP8emnn1JeXs64ceN49dVXUUrV28cHH3zAd999x1133QVYLzpu3LiR06dP\nM336dHbt2sWkSZPIysoiISGBF198kTfffJPp06cza9Ystm3b1uCxTJgwgYSEBH744QduuOEG7rvv\nvgY/M3s/89dff53XXnuNqqoqBg0axDvvvIOnpyczZszgmmuu4eabb+bVV19l48aN/Oc//6m3n7Vr\n17Jo0SIMBgNff/01GzZs4N1332Xx4sVUVVUxZswY/v3vf2MwGLj99tvZunUr5eXlzJo1i7/97W8s\nXryY48ePM3HiRIKCgtiwYQPe3t6UlJQAsGrVKj777DOWLVvGvHnzcHd3Z/v27YwfP57HH3+cO++8\nk127dmE0Glm4cCEzZswgLS2NW265haqqKiwWCx999BGDBw9u9v+Lxnh7e9t+Li0t7XoDTrTWXerh\nEemu/7X6Jt1T7N692/bzwjW79HWvbGrTx8I1u5qNwcvLS2uttclk0rNmzdLr1q3TWmttNBp1YWGh\n1lrrnJwcPXDgQG2xWPThw4e1wWDQ27dv11prfe211+p33nlHa611bGys/u6777TWWv/xj3/U0dHR\nWmutn3vuOX3LLbdorbXes2ePjoiI0OXl5Xrp0qV64MCBuqioSGdnZ2tfX1/98ssva621vvvuu/UL\nL7ygtdZ67ty5OjIyUsfHx+v4+HidkpKis7KydEREhM7OztZGo1FPnDhRr169WmutNaA//PBDrbXW\nVVVVeuzYsTo7O1trrfUHH3xgiyUkJERXVFRorbXOz8/XWmv96KOP6meffdb2+QQEBOiCgoIGP7u5\nc+fqlStXaq21Pn36tO31m266Sa9Zs6bRfUyfPl3/8MMPWmuti4uLtdFo1IcPH7Z9XrV/rr2fpo7l\n4osv1rfffnuDcdZm72eem5tre8/DDz+sFy9erLXW+uTJk3rgwIF648aNevDgwXWO+2y1P8vdu3fr\n6dOn66qqKq211rfffrtevnx5nc/OZDLpiy++WO/cuVNrrXX//v11Tk6ObXs1v6taa71y5Uo9d+5c\n2+czbdo0bTKZtNZaP/TQQ7bfyfz8fD148GBdUlKi77jjDv3uu+9qrbWurKzUZWVl9WK+7rrrbL9n\ntR81sZ7tv//9rx4yZIgOCAjQmzZtavSzaEu1vzdqAMm6hd+3Xe4MwkXDweJjHR1Gh2jJX/ptqby8\nnISEBLKyshg2bBiXX345YP3j4s9//jMbN27EycmJrKwsTp06BWC7HgAwcuRIMjIyKCgooKCggIsu\nugiAX//616xbtw6AH374gTvvvBOAoUOH0r9/f/bv3w/AxIkT8fHxwcfHBz8/P6688koAYmNjbWcz\nUL/E9MknnzBhwgSCg60djufMmcPGjRuZOXMmBoOBa665BoB9+/axa9cu23GZzWZCQkIAiIuLY86c\nOcycOZOZM2e26nPcsGEDzzzzDGVlZeTl5REdHc2VV17Z4D7Gjx/Pvffey5w5c/jVr35FeHi4Xfto\n6lgAZs+ebdd27PnMd+3axV/+8hcKCgooKSlh8uTJAPTp04fHHnuMiRMnsnr16ibP8Gr7+uuv2bZt\nG0lJSYD19653794ArFixgtdeew2TycSJEyfYvXs3cXFxdm23xrXXXmsrJa5fv541a9bYrhVVVFRw\n9OhRxo4dy5NPPklmZia/+tWvGjx7+PDDD1u036uvvpqrr76ajRs38sgjj3Spa0VdLkE4a0WGsW0u\nkgr71FyDKCsrY/LkySxZsoQFCxbwn//8h5ycHLZt24aLiwuRkZG2G3Tc3Nxs7zcYDJSXl5/z/mtv\ny8nJyfbcyckJk8l0Ttt0d3e3fVlorYmOjmbz5s311vv888/ZuHEjn376KU8++SSpqan11omOjmbb\ntm1ccsklje6voqKCP/zhDyQnJxMREcHChQttn1VD+3jwwQeZNm0aa9euZfz48XzxxRe4u7s3e1xN\nHQuAl5dXs9sA+z7zefPm8fHHHxMfH8+yZcv49ttvbe9JTU2lV69eHD9u/7B0rTVz587lH//4R53X\nDx8+zHPPPcfWrVsJCAhg3rx5jd4IVruEc/Y6tY9da81HH33EkCFD6qwzbNgwxowZw+eff87UqVN5\n9dVX6/27zp49m3379tXb97333svNN9/c6PFddNFFpKenk5ubS1BQUKPrdSZdbhSTk8XAMScTJsu5\nfTGIc+fp6cnixYt5/vnnMZlMFBYW0rt3b1xcXNiwYQNHjjTdbt7f3x9/f3/bSI7adekLL7zQ9nz/\n/v0cPXq03v+8LTV69Gi+++47cnNzMZvNvP/++1x88cX11hsyZAg5OTm2L1Wj0UhaWhoWi4Vjx44x\nceJEnn76aQoLCykpKcHHx4fi4mLb+x966CHuv/9+Tp48CUBVVRVvvPFGnX3UfFkFBQVRUlJiu9jc\n2D4OHTpEbGwsDzzwAElJSXZf2GzsWBqyevVqHnroIbu225Di4mJCQkIwGo11/i1//vln1q1bx/bt\n23nuuec4fPiwXdu79NJLWbVqFdnZ2QDk5eVx5MgRioqK8PLyws/Pj1OnTtnOOoF6/xZ9+vRhz549\nWCwWVq9e3ei+Jk+ezIsvvoi18gLbt28HID09nQEDBrBgwQJmzJhR5wy1xocffsiOHTvqPRpKDgcP\nHrTt45dffqGyspJevXrZ9Xl0Bl3uDEJpZ0xKkZm7m8jeLTvFFK2XmJhIXFwc77//PnPmzOHKK68k\nNjaWUaNGMXTo0Gbfv3TpUn7zm9+glKpzkfoPf/gDt99+O7GxsTg7O7Ns2bI6f8Wei5CQEJ566ikm\nTpxou0g9Y8aMeuu5urqyatUqFixYQGFhISaTibvvvpvzzjuPm266icLCQrTWLFiwAH9/f6688kpm\nzZrFJ598wosvvsjUqVM5deoUl112GVprlFK2C/E1/P39ufXWW4mJiaFv3762MorZbG5wH4888ggb\nNmzAycmJ6OhopkyZwokTJ5o95saOJTq6fnny0KFD+Pr6nuOnC48//jhjxowhODiYMWPGUFxcTGVl\nJbfeeitLly4lNDSU559/nt/85jd88803zV6gHT58OE888QSTJk3CYrHg4uLCkiVLOP/880lMTGTo\n0KFEREQwfvx423tuu+02rrjiCkJDQ9mwYQNPPfUU06dPJzg4mFGjRtkuWJ/tkUce4e677yYuLg6L\nxUJUVBSfffYZK1as4J133sHFxYW+ffvy5z//+Zw/H4CPPvqIt99+GxcXFzw8PPjwww+71IVqVZPd\nuoo+EX117yeC+VfcAi5JrD9SpLvZs2cPw4YN6+gwRDd000038cILL9iu0Yjuo6HvDaXUNq31qJZs\np+uVmAzW8c7pOfVP/YQQ9nv33XclOYgmdbkSk5OzB71NJg4V2lfXFEJ0vPnz5/Pjjz/Wee2uu+7i\nlltu6aCIhD26XIJwdXYitAoOlWd3dChCCDstWbKko0MQ56DLlZhcnZ3wrfIkw1JOV7t+IoQQXUmX\nSxAuBgOGqkDKFZwskdbfQgjhKF0uQbg6KyoqrXeGpp/Y2sHRCCFE99XlEoSTUhix3kCVfvKXDo5G\nCCG6ry6XIADwi8bfbCY9/0BHR9IjyHwQdcl8EG2nu88H8e233+Ln52f7vXzsscc6OqQW6XKjmAB8\neoXTv9hMeklWR4fSI8h8EHXJfBBtp7vPBwHWNjKfffZZu+2vLXXJM4iIXp4EVrmRbirs6FDa17oH\nYem0tn2se7BFIYwdO5asLGtiLikp4dJLL2XEiBHExsbyySefANYzg2HDhnHrrbcSHR3NpEmTbM36\ntm3bRnx8PPHx8XWGPlZUVHDLLbfY/gLfsGEDYJ2bYObMmVx++eVERkby0ksv8c9//pPExETOP/98\n8vLymoz3/fffJzY2lpiYGB544AHb697e3tx3333Ex8ezefNmtm3bxsUXX8zIkSOZPHmyra3F4sWL\nGT58OHFxcVx//fVkZGTwyiuv8MILL5CQkMD333/fovkgkpKSiImJ4bbbbrONwjt7HwDfffed7a/O\nxMREiouL65xx1Z4P4vvvv69zptLYsUyYMIG7776bUaNG8a9//avRz8zez/z1118nKSmJ+Ph4rrnm\nGttf8jNmzLAlyFdffZU5c+Y0uJ+a+SBefvllJk6cCFhv3hs9ejQJCQn87ne/w2w2A3D77bczatQo\noqOjefTRR22fW818EDXvrz3/wqpVq5g3bx5gPZP7/e9/z5gxY/jTn/5EaWkpv/nNbxg9ejSJiYm2\n3920tDTb/uPi4jhwoIdXKVraH7yjHyNHjtTvbzmi7392vI5ZFqNPlzfea747qNPXfe0DWr81tW0f\nax9oNgaZD0Lmg5D5IM5oyXwQGzZs0IGBgTouLk5fccUVeteu5udfaQs9dj4IgH6Bnmyt7AMUkn56\nL4Fh4zo6pPYx5akO2a3MByHzQch8EGe0ZD6IESNGcOTIEby9vVm7di0zZ87sUmclXbPEFOhJcWU/\nANKPy1BXR6u5BnHkyBG01rbSUO35IHbs2EGfPn0anQ/iXOdtOHtbjpwPoqZtc2pqKuvXrwesczXM\nnz+fX375haSkpAb3VzMfRFNq5oNYtWoVqamp3HrrrXXmgzh7Hw8++CBvvPEG5eXljB8/3u52300d\nC7T9fBAvvfQSqampPProo3XmX2jNfBA1se/bt4+FCxfa5oP4+uuvSUlJYdq0aW02H0TNvo4ePcqw\nYcO48cYbWbNmDR4eHkydOpVvvvmm3j5mz55tK//VfjR03cnX19dW9po6dSpGo5Hc3Fy7P5OO5tAE\noZS6Qim1Tyl1UCnVaLFbKXWNUkorpezqNBji584J80A8LBbScxvudS/answHIfNB1CbzQTQ/H8TJ\nkydt+/j555+xWCwyHwSAUsoALAEuBzKBrUqpNVrr3Wet5wPcBWyxd9vOBicqfKKIMhpJL276S0m0\nLZkPQuaDqCHzQTRv1apVvPzyyzg7O+Ph4cEHH3wg80EAKKXGAgu11pOrnz8EoLX+x1nrLQK+BO4H\n/qi1Tm5qu6NGjdLJycnc+PpPhFTNY6ePF1/d3PTpfVcm80EIR5H5ILqvrjAfRBhwrNbzzOrXbJRS\nI4AIrfXnLd14RIAnPkZvTukqSqoa/itBCNE4mQ9CNKfDLlIrpZyAfwL32bHubUqpZKVUck5ODgAR\ngR7ocuvoiMMyN4QQndr8+fPrXdRdunRpR4clmuHIYa5ZQESt5+HVr9XwAWKAb6trcn2BNUqpq84u\nM2mtXwNeA2uJCawjmY5UhgEnSM9OITY41mEHIoRoHZkPomty5BnEVmCwUipKKeUKXA+sqVmotS7U\nWgdprSO11pHAT0C95NCY8ABPsquicNaa9OwdjohfCCF6NIclCK21CbgD+ALYA6zQWqcppR5TSl3V\n2u33C/QkQ4cRaTSSnn9u/WiEEEI0zqF3Umut1wJrz3rtr42sO6El2w7yduW0c19GGM3sL2t++J8Q\nQoiW6ZJ3UoP1jsmQAG/6mD3INJdSaa7s6JC6LWn3XZe0+2473b3d97Jly3Bycqpzw11MTAwZGRkd\nF1QLdNkEAdYL1R5V/liAI0Vyw5yj1LTa2LVrF4GBgZ32guOzzz5ru6t1wYIFdr+vpmOovc5OELXb\nfaempvLTTz/h5+fXom3ao6F23ykpKdxzzz089thjXHbZZXZvqzWtT9pSTbvv7du3M3DgwBa//1wS\nRHsfe3h4OE8++WS77rOtdOkE0S/Qk4qKPgByHaKdSLtvafct7b5bZvr06aSlpbFv3756y5r6/Xz4\n4YeJj4/n/PPPtzXBzMnJ4ZprriEpKYmkpCR+/PHHVsfXpJa2f+3ox8iRI23ta1/feEjf+/DdOnZp\ntF6y6YmGut52ebXb9j615Sk9b928Nn08teWpZmOQdt/S7lvafZ/RknbfS5cu1fPnz9fLly/XN998\ns9Za6+joaH348OFmfz9rfj/uv/9+/fjjj2uttb7hhhv0999/r7XW+siRI3ro0KENfq49ut13jYhA\nT/5nDifMZCL9tH3NzETLSbtvafct7b7PaEm77xo33ngjTz75ZJ3GhVu3bm3099PV1ZXp06cD1v9/\nvvzySwC++uordu8+086uqKiIkpKSOmdObalrJ4gATw7rEMYaTaQXH+3ocBzugdEPNL+SA9Rcgygr\nK2Py5MksWbKEBQsW1Gn37eLiQmRkZKPtvmtKTOeivdp913RAre3zzz9n48aNfPrppzz55JOkpqbW\nW6em3fcll1zS6P5q2n0nJycTERHBwoUL67T7PnsfDz74INOmTWPt2rWMHz+eL774And392aPq6lj\ngbZv9/3xxx8THx/PsmXL+Pbbb23vaU2773/8o067Nlu7761btxIQEMC8efParN332R2Dhw0bxpgx\nY/j888+ZOnUqr776ar1/19mzZzdYLrr33nsb7OgK4OzszH333cfTTz/d4PKzubi42I6ldrt8i8XC\nTz/9ZNfvQlvo0tcgIgI9OI0v/UyKjKp8TJbOceGtu5J239LuuzZp9918u+/a5s2bx1dffUVNuyB7\nfz9rmzRpEi+++KLtec1c8Y7SpROEj7sLAZ6u9NK+GNFklWQ1/ybRKme3+05OTiY2Npa3337b7nbf\nNX15av7nBGu7b4vFQmxsLLNnz27zdt/x8fGMHDmyyXbfDzzwAPHx8SQkJLBp0yZbK+6aC+e1232v\nXr3adoF46tSp3HHHHVx22WVER0czYsQIioqK6uyjdrvvyZMn12v3ffY+Fi1aRExMDHFxcbi4uDBl\nyhS7jrmxY2lIW7X7Hj9+vO3fvqbd91tvvVWn3Xftf+vG1G73HRcXx+WXX86JEyeIj4+3tfu+8cYb\nG2z3XXORuqbd97hx4+qU1s72yCOPYDQaiYuLIzo6mkceeQSwlrJiYmJISEhg165dzX7pt4SrqysL\nFiywJUB7fz9rW7x4McnJycTFxTF8+HBeeeWVNouvIQ5r9+0oNe2+a1z10g/MMv6dZwOPsXjiYib2\nm9iB0bU9afctHEXafXdfXaHdd7uICPCkuMJ6AS89Ty5UC2EvafctmtOlL1IDhAd6sHdPX3qbTKRn\n17+AKIToePPnz683Zv+uu+7illtu6aCIhD26fILoF+jJd+YQoowm0gtbfqu+EMLxOuvd96Jp3aLE\nlKH7MKDKyOHyHLsuhnU13fGYhBCO0ZbfF10/QQR6UoEb4XhSqo2cKjvV0SG1KXd3d06fPi1JQgjR\nLK01p0+fbrP7JLp8iSnU3x2lIMApGMgjvSCdvl59OzqsNhMeHk5mZqZt7LQQQjTF3d3d7jvvm9Pl\nE4Sbs4G+vu4YLOFAHumFhxgXNq6jw2ozLi4uREVFdXQYQogeqMuXmMBaZjpuDMPXbCY9d3fzbxBC\nCNGs7pEgAjxJLe/NAKOJ9Lz6PVKEEEK0XPdIEIEebCsNYqDRSHpJZkeHI4QQ3UL3SBABnhzXvYg0\nafLN5eRX5Hd0SEII0eV1iwTRr5cnFpwIc7b2nU8vTO/giIQQouvrFgkiIsATAD/n6p5MkiCEEKLV\nukWC6O3jhquzE1WqPx4WC+n5rZ9HVggherpukSCcnBTh/h4cMocQaTSRnruno0MSQogur1skCIDw\nQE9SK4IYYDSSXpTR0eEIIUSX120SRL9AD7YU9WJAlZGTVQWUGks7OiQhhOjSuk2CiAjw5FiFO1HK\n2qTqcKF98+AKIYRoWPdJEIHWkUyh7tZ5aGUkkxBCtE73SRDVQ1293KNw1pr0AkkQQgjRGt0mQfSr\nPoPIdoqgn9FEev7+Do5ICCG6tm6TIPw8XfBxd+aQDrH2ZJIEIYQQrdJtEgRYy0y7KoKJqjJyrCyb\nKnNVR4ckhBBdVrMJQinVRyn1plJqXfXz4Uqp3zo+tJaLCPRgW3EAA0wmLGiOFB3p6JCEEKLLsucM\nYhnwBRBa/Xw/cLejAmqNiABPDheYiHIPAmQkkxBCtIY9CSJIa70CsABorU2A2aFRnaN+vTypNFkI\n845CaWQkkxBCtII9CaJUKdUL0ABKqfOBQns2rpS6Qim1Tyl1UCn1YAPLf6+USlVK7VBK/aCUGt6i\n6M9SM9S1wjOKULNZziCEEKIVnO1Y515gDTBQKfUjEAxc29yblFIGYAlwOZAJbFVKrdFa1540+j2t\n9SvV618F/BO4omWHcEZEoAcAJ5zDGFBVJdOPCiFEK9iTINKAi4EhgAL2Yd+Zx2jgoNY6HUAp9QEw\nA7AlCK11Ua31vag+SzlX4dVnEOmWUAYYjWwpPobZYsbgZGjNZoUQokey54t+s9bapLVO01rv0lob\ngc12vC8MOFbreWb1a3UopeYrpQ4BzwAL7Am6Me4uBnr7uLG7qjcDq4xUaRNZJVmt2aQQQvRYjSYI\npVRfpdRIwEMplaiUGlH9mAB4tlUAWuslWuuBwAPAXxqJ5TalVLJSKjknJ6fJ7UUEepJW7EmUtp41\nHCw42FahCiFEj9JUiWkyMA8Ix3ptoEYx8Gc7tp0FRNR6Hl79WmM+AF5uaIHW+jXgNYBRo0Y1WYaK\nCPBga0Y+Q/z74UsJnxz8hEv6XWJHuEIIIWpr9AxCa71caz0RmKe1nljrcZXW+r92bHsrMFgpFaWU\ncgWux3qx20YpNbjW02lAq+cKjQj05ERhOW6Bg7mhQvPNsW9kuKsQQpyDZq9BaK0/UkpNU0r9SSn1\n15qHHe8zAXdgvcluD7BCa52mlHqsesQSwB1KqTSl1A6so6XmtuJYAOtQV4uGIq/+3JidhbvBjaVp\nS1u7WSGE6HGaHcWklHoF6zWHicAbwCzgZ3s2rrVeC6w967W/1vr5rpYEa4+aeSFOufRjiNnEr8Iv\nYUX6Z8xPmE9fr75tvTshhOi27BnFNE5rfTOQr7X+GzAWOM+xYZ27mnshMrBOHHSz71C01ryz+52O\nDEsIIbocexJERfV/y5RSoYARqr99O6EQPw+cnRSppjDw7kvYjpVMiZrCyv0rKay06wZwIYQQ2Jcg\nPlVK+QPPAr8AGcB7jgyqNQxOilB/DzIKzHDRH+HoJm7xHUa5qZz3977f0eEJIUSX0WSCUEo5AV9r\nrQu01h8B/YGhta8jdEb9Aj05ll8OI+aCXz/O2/IWF4VdxHt73qPcVN7R4QkhRJfQZILQWluw9lOq\neV6pte70dZqIQA8y88rA2RUmPADHt/Nb36HkV+az+sDqjg5PCCG6BHtKTF8rpa5RSimHR9NGwgM8\nOV1aRWmlCeKuh16DGbH1PyQEx7M8bTlGi7GjQxRCiE7PngTxO2AlUKmUKlJKFSulipp7U0eqGep6\nLL8MDM4w8SHI2cNvfYZyvPQ4X2R80cERCiFE52fPjXI+WmsnrbWr1tq3+rlvewR3riICrENdj+VV\nX28YfjX0ieGiX1YyyG8gb+16C61b1ThWCCG6PXvOILqcfjVnEHll1hecnGDiwzjlZ3CL92AO5B/g\n+6zvOzBCIYTo/Lplggj0csXT1WAtMdUYMgXCRjIl5XP6evbhzdQ3Oy5AIYToArplglBKERHgeeYM\nwvoiXPIILkVZzPMezC/Zv7Aje0fHBSmEEJ1cc/dBGJRSe9srmLYUEehx5hpEjQETIPJCrt71Jf6u\nfry5S84ihBCiMc3dB2EG9ikngPUyAAAgAElEQVSl+rVTPG0mItCTY/lldS9GV59FeJbmcKNXFN8e\n+5aD+TKhkBBCNMSeElMAkKaU+loptabm4ejAWisiwJOyKjN5pVV1F/QbA4MnccOe7/EwuEsrcCGE\naESz7b6BRxwehQPU3AtxNK+MXt5udRde8hf8X72IazxG8kH6Wu5IuIMQ707bf1AIITqEPfdBfAfs\nBXyqH3uqX+vUaoa6Hsoprb8wJB6Gz+Dmgz8Dmrd3v92+wQkhRBfQbIJQSl2HdYKga4HrgC1KqVmO\nDqy1BvX2Jszfg5XJxxpeYeLDhFSUMNUthI8OfERBRUH7BiiEEJ2cPdcgHgaStNZzqycOGk0XKDsZ\nnBS/HtufLYfz2Huygc4gwUMgbja3HN5pbQW+T1qBCyFEbfYkCCetdXat56ftfF+Hmz0qAjdnJ5Zv\nOtLwChc/wKCqSia4BPPenvcoM5Y1vJ4QQvRA9nzR/08p9YVSap5Sah7wOWfNM91ZBXi5MjMhjI+3\nZ1FY1kAH18AoGHEzvz22m4LKAlYflFbgQghRw56L1PcDrwJx1Y/XtNYPODqwtnLzuP6UG82saOxa\nxIV/JKHKwgiDL+/sfgeLtrRvgEII0UnZcyf1Bq31f7XW91Y/utSf2dGhfiRFBvDOT0cwWxro4OoX\nBkn/x6yTGWSVZJGam9r+QQohRCdkz53UFqWUXzvF4xBzx0VyNK+Mb/dlN7zCBfcwoQpcUDJXhBBC\nVLPnGkQJkKqUelMptbjm4ejA2tLk6L708XVj2aaMhlfwDsYn6beMLyvjy8P/kzKTEEJgX4L4L9Zh\nrRuBbbUeXYaLwYk5Y/rz/YFcDuWUNLxSwhwmlZRysjyHlJyU9g1QCCE6oWavQQCTtNbLz360U3xt\n5obR/XA1OPHO5kaGvAYPYYLvIFw0rD+yvn2DE0KITsieaxD9lVKu7RSPwwT7uDEtLoRV2zIpqTQ1\nuI5P3A3WMlP6OikzCSF6PHtKTOnAj0qpR5RS99Y8HB2YI8wdF0lJpYmPtmU2vELMNUwqq+BkRa6U\nmYQQPZ49CeIQ8Fn1uj61Hl1OQoQ/8eF+LN+cUXeeiBo+fZjYexSuGtbLaCYhRA9nz41yf9Na/w14\ntubn6udd0txxkaTnlPLDwdwGl3vH38i4sjLWp38uZSYhRI9mTzfXsUqp3VhbfqOUildK/dvhkTnI\ntLgQgrxdWd7YkNeh05hcYeZUZb6UmYQQPZo9JaZFwGSsTfrQWu8ELnJkUI7k5mzghtH9+HpvNkdP\nN9Ccz82bCf0m4qo1XxzuEi2nhBDCIezqyqq1PruRkdkBsbSbG8f0w0kp3t3S8JBX7/g5jCsr58v0\ntVJmEkL0WPYkiGNKqXGAVkq5KKX+COxxcFwOFeLnwRXRfflw6zHKqxrIdQMmMNnkzKmqQikzCSF6\nLHsSxO+B+UAYkAUkVD/v0uaOi6Sw3MjHO7LqLzQ4M2HgdGuZ6eCa9g9OCCE6AXtGMeVqredorfto\nrXtrrW/SWp+2Z+NKqSuUUvuUUgeVUg82sPxepdRupVSKUuprpVT/czmIc5EUGcDQvj4s39TwkFfv\nhDmMLyuX3kxCiB7LYTPDVbfpWAJMAYYDNyilhp+12nZglNY6DlgFPOOoeBqIj3njItl7spifD+fV\nXyE0kUnKl1OmYikzCSF6JEdOHToaOKi1TtdaVwEfADNqr6C13qC1rhlK9BMQ7sB46pmREIafhwvL\nN2fUX6gUE4bOwtWi+WLfR+0ZlhBCdAqOTBBhQO3RT5nVrzXmt8A6B8ZTj4ergeuTIvgi7RTHC8rr\nLfeOv5Hx5eWsP7JeykxCiB6nuW6uFyul4qp/vk4p9ZJS6h6llFtbBqGUugkYBTzbyPLblFLJSqnk\nnJycttw1N53fH4vWvLflaP2FgVFMdg8l21xGSvbONt2vEEJ0do0mCKXUEuAJ4A2l1LvAjcAuYATw\nlh3bzgIiaj0Pr37t7P1cBjwMXKW1rmxoQ1rr17TWo7TWo4KDg+3Ytf0iAj25dGgf3v/5KBXG+kNe\nJwy/0Vpm2v1em+5XCCE6u6bOICZqrS/Eetf0FOAarfUrwM1AnB3b3goMVkpFVbcLvx6oM2ZUKZUI\nvIo1OTQyH6jjzRsXyenSKj5POVFvmVfcbMZXVLI+81spMwkhepSmEkQFgNa6AjhSPTcE2jom1Njc\nhrXWJuAO4AusN9at0FqnKaUeU0pdVb3as4A3sFIptUMp1SE3HYwf1IuBwV68vTmj/kLPQCb7DCbb\nUkHKqe3tHZoQQnQY5yaW9a6e90HV+pnq53bVebTWa4G1Z73211o/X9aycB1DKcWvz+/Pwk93k3a8\nkOhQvzrLJ8TNxXXb43yRupSEviM7KEohhGhfTZ1BvI513gfvWj/XPH/D8aG1r5mJYbg6O7Fi69lt\np8Br2AwuqDSy/sRmKTMJIXqMRs8guvKcD+fC39OVK6L7snp7Fg9NHYa7i+HMQhd3JvWK45uyvew8\nvoXEsLEdF6gQQrST5oa5TlFKbVRK5VY/vlNKTW2v4Nrb9UkRFFWY+CLtZL1lExJuw9WiWb/zzQ6I\nTAgh2l9Tw1xvBR4HFgIDqh9/AxYqpW5rl+ja2fkDetEv0JMPfm6gzDTwUi4wwfqcbVJmEkL0CE2d\nQdwDTNJaf6O1Lqp+fIN1yOs97RNe+3JyUlw3KpzN6afJyC09eyGTeo8mGxM7j37XMQEKIUQ7aipB\nKK11vS529nZy7apmjYzAScGK5PpnERNGzbfeNCdlJiFED9BUgihSSsWf/WL1a8WOC6lj9fVzZ+KQ\n3qzalonJXLeU5BU2kgssLnyZlyplJiFEt9dUgrgPWKOUWqiUurL68TfgE+DeJt7X5c1OiiC7uJJv\n99Xv+zQ57EKylYWdB2W+aiFE99ZogtBa/wCMqV5nXvXDCTi/elm3NXFob4K83figgXsiLk5aYC0z\npS5r/8CEEKIdNXUnNVrrk0qpvwODql86WN16o1tzMTgxa2Q4r3+fTnZRBb193W3LvHoN4kInL74s\n3MefLGacnAxNbEkIIbqupoa5OiulnsE6p8Ny4G3gmFLqGaWUS3sF2FFmJ0VgtmhW/ZJZb9mkiMvI\ndoIdaR90QGRCCNE+mroG8SwQCAzQWo/UWo8ABgL+wHPtEVxHigryYnRUICu2Hqs3Z/XFSXfgadG8\nt20xWORitRCie2oqQUwHbtVa20Ysaa2LgNuBbns3dW3XJ0WQcbqMLWfNWe3lE8KcPmP5QpWx94en\nOig6IYRwrKYShNZn/+lsfdEM1Hu9O5oSE4KPuzMfNnCxet6lz+GDEy/tXg75Ge0fnBBCOFhTCWK3\nUurms1+snh50r+NC6jw8XA3MSAhlbeoJCsvrToHh6+bHLcN+zXceruz85FYpNQkhup2mEsR8YL5S\n6lul1PPVj++ABVjLTD3C9Un9qDRZWLOj3mypzEn8A4EGT16syIBkubtaCNG9NHUfRJbWegzwGJBR\n/XhMaz1aa13/27KbignzIzrUt8F7IjxdPPm/xPls8XDn542PQ97hDohQCCEco8l23wDVzfperH58\n3R5BdTazkyJIO17ErqzCesuuGzqbPh5BLPbzQq+5Q0pNQohuo9kEIWBGfBhuzk4NXqx2M7jxu4Q/\nsNPVme9PJUupSQjRbUiCsIOfpwtTY0P4eEcW5VXmestnDppJhE8EL/aNwPLlX6XUJIToFiRB2Gl2\nUgTFFSbW7TpRb5mLkwu3x9/OXir50tMDPjn3UpPWmpX7V3Ig/0BrQxZCiFaRBGGnMVGBRPbybPBi\nNcDUqKkM9BvIkpAIzEd+gK1vnNN+Fv2yiMc2P8ad39xJmbGsNSELIUSrSIKwk1KK65Ii+PlwHuk5\nJfWWG5wM3JF4B4cr8/hsQBJ89SjkpbdoH2+mvslbu97iwrALOV5ynH9u+2dbhS+EEC0mCaIFZo0I\nx+CkWJFcv4EfwKX9LmVY4DBe9gCjk3OLSk0r9q1g0S+LmBI1hZcufYk5w+bw4b4P2XJiS1seghBC\n2E0SRAv09j0z25zRXP+LXynFnYl3klV2iv+OuhaO/AhbX292u2vT1/LET09wUfhFPHnBkzgpJxaM\nWEB/3/48uulRKTUJITqEJIgWuj4pgtySSjbszW5w+QVhF5DYO5HX8ndSMfBS+Gphk6WmjZkbefiH\nhxnZZyTPX/w8Lk7WTuoezh48Pv5xKTUJITqMJIgWmjAkmN4+bg3eEwFnziKyy7P5cPgEaKLUlHwy\nmXu/vZfzAs/jxUtexN3Zvc7yxN6J3DT8Jik1CSE6hCSIFnKunm1uw75sThY2PLleUt8kxoaM5c0D\nqyi97FFrqWnLy1CrOW7a6TTu+OYOQr1DeeWyV/B29W5wW3cm3kl/3/789ce/UmosdcgxCSFEQyRB\nnIPZSRFYNLy2sfHS0Z2Jd5Jfmc+7rmYYdDl88Wd4bjC8N5v0L//C7f/7LX4uPrx2+WsEuAc0up2a\nUtOJ0hO8sO0FRxyOEEI0SBLEOejfy4s5Y/qxdNNhkjPyGlwnNjiWiRETWZ62nMIZ/4Jpz8Ogyzme\nf4jbjn6EU0URr+37hb5vTYfVv4efX4fj28FUVW9bUmoSQnQE1cCcQJ3aqFGjdHJyckeHQUmlickv\nbMTV2Ym1Cy7Ew9VQb539+fuZtWYWv439LXeNuIvc8lzmrptLfkUeS2P+wJCCk5C5DbKSoTTH+iaD\nG4TEw6WPQNRFtm2Vm8q59tNrMZqN/HfGf/Fy8WqvQxVCdANKqW1a61EteY+cQZwjbzdnnp0Vx+Hc\nUp5bv6/Bdc4LOI8roq7gP3v+w+HCw/z+y9+TU57Dvy97mSFxv4aL7ocbP4A/HoC7UmDWW5D0f1B8\nElbeAqW5tm3VLjX9M1lGNQkhHE8SRCuMGxTETef3460fD7O1kVLT/IT5VJmrmP3ZbA4VHmLRxEUk\n9E6ou5JSENAfYq6BK/5uTRoVhbD2j3VWS+ydyK+H/5oV+1fw04mfHHVYQggBSIJotYemDCPM34M/\nrUppsNNrf9/+zBw0k0pzJc9c9AzjQsc1v9E+0TDhAUhbDWkf11l0Z+KdRPpG8uiPj8qoJiGEQ0mC\naCUvN2eeuabpUtPDYx7m05mfcnn/y+3f8Ph7ICQBPr+vTqnJ3dndVmp6Pvn51oYvhBCNcmiCUEpd\noZTap5Q6qJR6sIHlFymlflFKmZRSsxwZiyM1V2pyMbjQz7dfyzZqcIaZ/26w1JTQO4Gbh9/Myv0r\n2Xx8c2tCF0KIRjksQSilDMASYAowHLhBKTX8rNWOAvOA9xwVR3upKTXdv3Jng6Wmc9JEqemOxDuI\n9I1k4aaFUmoSQjiEswO3PRo4qLVOB1BKfQDMAHbXrKC1zqhe1uUncvZyc+aZWXHc+PoWnv1iH3+9\n8uxceI7G3wN7PrOWmiIvAK8g4Eyp6eZ1N/P4T49z1YCr7N5kH68+RPlF4aSkwiiEaJwjE0QYULth\nUSYwxoH763DjBgbx6/P7s3TTYa6I6cvoqMDWb9TgDDNfhtcutiaJ65bbFiX0TmBe9DyWpi3l8/TP\nW7RZbxdvooOiiQ2KJTYolrjgOII8glofrxCi23BkgmgzSqnbgNsA+vVrYS2/nT04ZSgb9mXzp1U7\nWXfXRQ3eQNdifYbDxQ/AN49by03RV9sW3TPyHq6IuoIqc/07sBui0RwpOkJqTiqpuaks27UMkzYB\nEOIVYksWsUGxDOs1DA9nj9bHL4Tokhx2J7VSaiywUGs9ufr5QwBa6380sO4y4DOt9armtttZ7qRu\nyqZDudz4+hZuGR/Jo1dGt81GzSZ48zIoOAp/2ALewW2y2QpTBXvy9pCSk8Ku3F2k5qaSVZIFgEEZ\nGBI4hKsHXc1VA6/C08WzTfYphGh/53IntSMThDOwH7gUyAK2AjdqrdMaWHcZ3ShBADzy8S7e3XKE\nD28b2zalJoDsPfDqRTBkClz3dttsswG55bmk5aaRkpvCD1k/sPv0brxdvJk5aCY3DL2h5SOyhBAd\nrlMlCACl1FRgEWAA3tJaP6mUegxI1lqvUUolAauBAKACOKm1bvJP7q6SIEorTVzxr40YlGq7UhPA\n98/D14/BtcvqlJocRWtNSm4K7+15j/UZ6zFrMxeGX8icoXMYGzoWpZTDYxBCtF6nSxCO0FUSBMDm\nQ6e54fWfukSpyR7ZZdms3L+SlftWcrriNFF+Udww9AauGniVNA8UopOTZn2dzNiBvbh5bH+Wbcrg\n58MN92pqsZpRTZXFsPa+ttmmnXp79mZ+wnzWz1rP3y/4O17OXvx9y9+5bOVlPP3z0xwtOtqu8Qgh\nHEvOIBysptSUV1JFL283u97jpODOSwZzzcjwxleqKTXNWgoxv2p8PYsFTh+0thTPTIbjv0B5fguP\nonEpBs1/3DTrXcAMxJkhzgRxZkWMGcIsoGikDOXsDuPuhIQ51oaFQgiHkRJTJ7X7eBFv/XgYs8W+\nz3rvyWLSc0r4390XERXUSOnGbII3L4eCI3VLTSU5Z5JBVjJkbYfKQusyVx8ISwSfkDY4qrpyLFWs\nMp5is7mQ3eYSKrEea6ByIdbJm1iD9RFj8MZXVY+uzj1gTVhRF8OV/4LAqDaPSwhhJQmim8guquCy\nf37HeX18+PB3YzE4NfLXdfZeePVCCBsFPn2skw8VVpd5lMF6/0TYKAgfZf1v0Hng5PiqotFi5ED+\nAXbl7iIlJ4XU3FTSC89MzxrpG0lccBzxQbFMLyzA8+snwGKCSx6GMbdby2hCiDYlCaIb+e8vmdy7\nYid/mTaM/7twQOMr/rgYvnwE/CIgbOSZZBASD66d576F4qpi0k6nkZqTSkpuCqk5qZyuOE2Ydxh/\njZ/PuK3vwb611g62V70IIXEdHbIQ3YokiG5Ea82tbyfz/YFc1t11IQOCvRtbESqLwN2vfQNsJa01\nyaeSeWzzY2QUZXDVwKu432c4/usfhbI8GL/Aeve4i9zJLURbkFFM3YhSir9fHYu7i4H7V6U0fv1C\nqS6XHMB6fEl9k1h11Spujb2VtelrmbH3Nf531VPouOvhhxfg5fGQ8UNHhypEjyUJohPr7evOwquG\ns+1IPkt/PNzR4TiEm8GNBSMW8MH0DwjxCuH+n/7Gnb5OnLxuKWgzLJsGn94F5QUdHaoQPY6UmDo5\na6lpG98fyGHtXRcysLFSUzdgtph5d8+7vLT9JQxOBu6O/wPXZe7D6ad/g1dvGH0rODc9VLjcYmJv\nZS6HqgoYHXYB/QZNBs82anViqoSTqXBqF4QnWefrEKKLkGsQ3VR2UQWXv7CRgcFerPz9uMZHNXUT\nx4qP8fjmx9l8YjOJvRNZOPA6BnzzlPXLuRYLcNjFmVQ3N1LdXEl1c2O/qwvmWvdUXFhWzo3am3F9\nknCquYDfN6bZRIPWkJdePVx4m3XI8MlUqN01N/JCGPM7OG+KjLwSnZ4kiG5s9fZM7vlwJw9PHcat\nFzUxqqmb0Fqz5tAantn6DOWmcm6Lu42ZEZexp2A/qafTSDm9m7S8PZRUz6bn7eJFdOAw4noNJ7ZX\nNP08+/LFng9YkbWB0+YKIk2a6wvzmVFcireTC/SNqx7xNdL6cPevTgTVySBr25kbCl08ITTxzCix\n4KGwbx1sfQMKj1lHkCX9H4y4ue3OVoRoY5IgurGeVGqqLbc8l6d/fpr/ZfzP9ppBGTgv4DzrZEfB\nscQFxRHpF9ngDHlGs5EvjnzB+3veIyU3FS8nN2a4h3FDSRmRx3eBseysdyjoPazukOHgoQ2fIZhN\nsH8dbHkVMr633hkedx2M/p31LEWITkQSRDfX00pNtW3K2sShwkPEBMUwNHDoOU1klJqTynt73+N/\nGf/DZDExPnQcc/pewPgKI05VxRA6AkITwM2n5QGeSoOfX4OdH4KpHPpfAGNugyHTpPwkOgVJED3A\nx9uzuPvDHT2m1OQIueW5rNy/khX7VpBbnkt/3/7MHDSThOAEhvca3rqJkcryYPs78PMb1rvafcMh\n5mqwN6EpBeddAWEjzj0GIRogCaIH0Fpz2zvb2Li/Z5WaHMFoNrL+yHre2/seKTkpgLV8Nch/ELHB\nsbb5ugf4DcDg1ML5PCxm63WKLa/AkR+tF73tokE5WVuOXPIwuEobddE2JEH0EK0pNVksGqWQiX7O\ncrr8tLV3VHUbkF25uyg2FgPg5eJFdK9o2zWP2KBYenv2dkwgFYXw1UJIfgv8+8H0RTDoUsfsS/Qo\nkiB6kJpS05+nDuW2iwY2uI7WmuOFFew4WsCOY/nsOFZAalYhHi4G4iP8SYjwt/433J8AL9d2PoLO\nzaItHCk6Qmpuqm2+7n15+zBpEwB9PPsQFxxnO8todWnqbEc2wZo7ra3a42+EyU/KCCnRKpIgepCa\nUtN3+3NYu+BCBvX2prjCSEpmITuOFbD9aAE7jhWQW1IJgKuzE9GhvsSH+1NeZWbHsQL2ZxfbKh+R\nvTzPJIwIf4aH+uLm3EbTpHYTleZK9pzeQ2puqvWRk0pmSSYATsrJWpoKirUljnMqTdVmrICNz8KP\ni6zDcKc8DTHXyNwZ4pxIguhhsosrmPTCRnzdXXB1duJQTontC39AkBcJEf4k9PMnPtyfYSG+uDrX\nHQZaUmkiJdOaSHYes/73VFF1QjE4MSzUl+Ehvrg529+RJdjHjcQIf2LD/fBxd2mzY+2s8iry6rQ1\nT81NpbjKWprydPYkOshamooLiiM2+BxLUyd3Wc8mjv9ivYA97Xnwa2IyqVYwW8ykF6aTmpvKseJj\nDPQfSFxQHBE+EVKW7OIkQfRA61JP8MTnexjS16dOycjP89y+nE8UlleXpKyP/aeK7Z7oSAPFFdYS\njFIwKNjblqQSIvwZ0scHZ0P3bv+lta5TmkrNTWVf/j5MljOlqdrXMqJ7RdtXmrKYrRe8v3nCehH7\nsoUw6retnt8juyz7TAv23FTSctMoM1nvDVEodPXET/5u/sQExRAXFEdMUAyxQbH4u/u3at+ifUmC\nEB2uoKyKnZmFtuseOzMLySu1tqdwd3EiNszPmjQiAkjs50+of/dv511prmRv3t46c2HULk3V/JVe\nkzgG+g1svDSVnwGf3QOHvoGIMdaZ+HoPsyuOMmOZdU6O6vJYSm4K2WXZADg7OTMkYEidElmYTxjp\nBemk5KbYzpIOFRyyJY1+Pv3qjPYK8Qrp/mcZVeUEeofg5IB7W0qNpZSbytt8uzWCPYMlQYjORWvN\nsbxytldfJN9xrIC040VUmSwAnD8gkHnjorhsWO9uf3ZRW01pqubLOjU3laKqIgA8nD2so6aq7xKP\nDYqlj1efM2/WGlI+hP89aG0H4t//zF3fYSMhJA6zwZVDhYds207JtX65W7T1cw/3DrdtPyYohmG9\nhuFmaH7O9FJjKWm5abakkZqTSnZ5tkM+o87Kx6KJcfIkxieSuNAxxA66kl5B57VoG0aLkf35+23/\nPqm5qRwudGzH5l3zdkmCEJ1flcnC3pNFfH8gl/e2HCWroJwwfw9+PbY/1ydF4O/Z80ZU1S5N1SSN\nvfl7baWp3h69bX+txwXHWUtTlaWw8z3ITObkiW3sqsojxc2NVHc30tzcKK/+Y97XxZvY4DjrIyiW\nmKAYAt3bbkTUydKTpOamkl+R32bb7BTMRji+AzI2QmEmOHtgCh/JQWMhqeUnOKDMtsaQYWaIcQ0g\nNmAocREXMWzQVNyrR51prckqyarzb7snbw+VZuv1vkD3QOKC4ogOim7Tf5ezzR46WxKE6FpMZgtf\n7clm2abD/JSeh7uLEzMTwpg7LpJhIb4dHV6HqjRXsi9vX53rGceKjwHW0tQAvwGEeoey9/Re21/x\nzsrAUGdfYqtMxOafIK60iH4mE8rdz9pKxCekIw/p3HkGnumP5Rfh2JFcRcet96EkL4WyXAgaYm2b\nEnc9uJ25MbWs7DR7DnzOrszvScnfS2pVPicM1rictWawdqaXiw+7LaXkaSMAbjgx3OBNjMGbOIMP\nsQYfQpWbtTSnFPQ7H2KvdchMinINQnRpe08WsXxTBqu3Z1FhtDAmKpBbxkdy2bA+Par81JSCigLb\nX6IpuSmcKj3FeQHn2a4bDA0ciquh+gzMYobc/dUty2s61BZ27AGcEw0l2VD9Fzdevet24g0b0fpZ\nFbWGYz9bBwLsWWP97IZMgdG3wYAJdiek3Jw9pBz4jF0nfyalOIM8UwXDzBBnglgTDDZDo8NHTBVQ\nmg0eATBirrVDsH9E646rFkkQolsoKKviw63HeHvzEVv56abz+3PZsN44dYIGhQoIC/CQ+0Tak6kK\nstPOzM+RmQynD1QvVBB0XnXSGGE9U7K34aLW1uS55RU4sRPc/GDEr61fzoFRDjucRmM58qM1lr2f\nW18bOt0650j/8a07ayrJRvn0kQQhug+zRfPVnlMs35TBpkOnOzqcOlwMiuEhvnXuNYkK8ur+o3g6\nk/J8yPql+pFsTRpluee2reCh1i/iuNmdo/9VwVHY+ib8stx6nH1irGczsdeCazPDoqvK4MSOM4k0\naxsUHkP9rUgShOie9p8qZs+Joo4OA7Amrn2nitlx1Nq6pKzKDICfh4vtTvSECD8SIgIIlBYm7Udr\nKDgCJ1LqzvzXHN9Q6De2c96hbiyH1JXWOUdO7apffrJYIHdfrWSQDKd2W+dzB2s/r7BRED4KNe4O\nSRBCtCezRXMgu7jezYU19xb2C/TkvD7eONn55aMUJEUGcu2oCPw82u5O9AOnivnPlqMcL3DcOPuu\nytfDhfhwa0IfGuKDSyuud9X0P6vpTJCRW9o2QWrN4IpULin6L4mlPwJwzP08QozHcDVX78PNz1pi\nq319xvvMnftyDUKITqC00kRqlrUn1o6jBWSctv9LospkIT23FA8XA78aEca8cZEM7nMOExhhTV7f\n7M1m+aYMfjiYi6uzEwOCOkH5pJPJLakkt8R6xuHm7ERM9c2c8RH+JEb4Ex7g0WjpsLjCSGpmIduP\nnfkDIaf4TP+zyF6edjdk0YkAAAlASURBVP9xYK9gSzbTK9cxtCqNncZwdloGksIgXHufR3z/QBKq\n4x4Y7F3nmp0kCCG6gbTjhSzflMHHO45TZbIwflAv5o2L4pKhve1q7V5YZmRF8jHe/imDY3nlhPi5\nc9P5/blhdD8peTVAa01WQbktodd0Pa6svpmzl5drddnQn+gwX07YOiQXcLCF/c/aWm5Jpe1speZR\n0+7G282ZuHA/W+yTY0IkQQjRXeSVVvHB1qO8s/kIJworiAj04ObzI7luVESDvbYOnCpm2aYM/vtL\nFuVGM6MjA5k3PpJJw2WYcEsZzRb2nSyu88V7MLvEtjzQy9VWlrImBL9OcYOnxaJJzy2tkzT2nCjC\nZNEceXq6JAghuhuT2cL63adYtimDnw/n4eFi4Orq8tPAYG++3nOK5Zsz+PHgadycnZiREMrccZFE\nh7by3gBRR1GFkT3Hiwjx8yAisPGyU2dTYTSTdryQUZG9JEEI0Z3tPl5UXX7KotJkIdDLlbzSKkL8\n3KtblUgZSTRMrkEI0UPkl1bxwdZjpGQWcGV8qJSRRLPOJUG0fc/aWpRSVwD/AgzAG1rrp85a7ga8\nDYwETgOztdYZjoxJiO4gwMuV2yc0PNWsEG3FYX9yKKUMwBJgCjAcuEEpNfys1X4L5GutBwEvAE87\nKh4hhBAt48hz0tHAQa11uta6CvgAmHHWOjOA5dU/rwIuVV3lyo8QQnRzjkwQYcCxWs8zq19rcB2t\ntQkoBHo5MCYhhBB26hJXtZRStymlkpVSyTk5OR0djhBC9AiOTBBZQO1m5uHVrzW4jlLKGfDDerG6\nDq31a1rrUVrrUcHBwQ4KVwghRG2OTBBbgcFKqSillCtwPbDmrHXWAHOrf54FfKO72rhbIYTophw2\nzFVrbVJK3QF8gXWY61ta6zSl1GNAstZ6DfAm8I5S6iCQhzWJCCGE6AQceh/E/7d3/6F213Ucx5+v\nrsJqreaaDA1jNiQx01v5A0tqFfVHQViwiRS0f6ygkSZBEmImRPbDIDBuKdYKrLXQEnGIotcfBU28\nc9fNVqR5M2xtg/LHLRa4vfrj87l1un7PvPfunHs83/t6wJfzPZ9zzuf7+ezDPZ99P9/veb9tbwe2\nzyq7umP/ELChn22IiIiFGbpfUks6CPx50O1YgNXAAtNdDZX0sz2WQh9h6fTzLbbnFTu+r2cQ/WB7\nKK9SS3pkvj9zH0bpZ3sshT7C0urnfD8zFLe5RkTE4ssEERERjTJBLJ4bB92ARZJ+tsdS6COkn10N\n3UXqiIhYHDmDiIiIRpkgFoGkKUm7Je1ayJ0Er1SSfijpgKQ9HWWrJN0j6Y/18YRBtvFYdenjNZKe\nqeO5S9KHB9nGXpB0iqRxSb+T9Liky2p528azWz9bM6aSlkl6WNJk7eNXa/mpknZIekLSz2uEi6PX\nlSWm/pM0BZxju1X3Wkt6DzAN/MT2mbXsm8DfbV8n6UrgBNtfGmQ7j0WXPl4DTNv+9iDb1kuSTgJO\nsr1T0gpgArgI2ES7xrNbPzfSkjGtKROW256WdDzwa+Ay4ArgNttbJX0fmLQ9drS6cgYRC2b7QUqI\nlE6dOT5+TPnjG1pd+tg6tvfZ3ln3XwD2UsLxt208u/WzNVxM16fH183A+yl5d2COY5kJYnEYuFvS\nhKRPD7oxfbbG9r66/zdgzSAb00ebJT1Wl6CGetllNklrgbcDO2jxeM7qJ7RoTCWNSNoFHADuAZ4E\nnq15d6A5P89LZIJYHBfafgcl/ern6rJF69XIvG1cwxwD1gGjwD7g+sE2p3ckvRa4Fbjc9vOdr7Vp\nPBv62aoxtX3Y9iglzcJ5wOkLqScTxCKw/Ux9PAD8kjJgbbW/rvPOrPceGHB7es72/voHeAS4iZaM\nZ12vvhW4xfZttbh149nUz7aOqe1ngXHgAmBlzbsDzfl5XiITRJ9JWl4vhiFpOfAhYM/RPzXUOnN8\nfAq4fYBt6YuZL8zqY7RgPOuFzZuBvba/0/FSq8azWz/bNKaSTpS0su6/Gvgg5VrLOCXvDsxxLHMX\nU59JejPlrAFKcMSf2v7aAJvUM5J+BqynRMPcD3wF+BWwDXgTJeruRttDe5G3Sx/XU5YiDEwBn+lY\npx9Kki4EHgJ2A0dq8Zcp6/NtGs9u/byEloyppLMoF6FHKCcB22xfW7+LtgKrgEeBT9r+91HrygQR\nERFNssQUERGNMkFERESjTBAREdEoE0RERDTKBBEREY0yQURERKNMEBFzIGm0MwS0pI/W6Ka9qPty\nSa/pRV0RvZTfQUTMgaRNlJDtm/tQ9xTzDAcvacT24V63JaJTziCiVSStlbRX0k01WcrdNdxA03vX\nSbqrRtl9SNLptXyDpD014cqDNbHKtcDFNZnMxZI2Sbqhvn+LpDFJv5X0J0nra0TQvZK2dBxvTNIj\ns5K4fB44GRiXNF7LLlFJMLVH0jc6Pj8t6XpJk8AFkq6riW8ekzT0eQziFch2tmyt2YC1wIvAaH2+\njRJSoOm99wKn1f3zgfvq/m7gjXV/ZX3cBNzQ8dn/Pge2UEIYiJI/4XngbZT/gE10tGVVfRwB7gfO\nqs+ngNV1/2TgaeBESmiW+4CL6mumhLoAeAPwB/63CrBy0P/22dq35Qwi2ugp27vq/gRl0vg/Ndzz\nu4Bf1Lj5PwBmArb9Btgi6VLKl/lc3GHblMllv+3dLpFBH+84/kZJOylxcN4KnNFQz7nA/bYPusTu\nvwWYCQ9/mBKFFOA54BBws6SPA/+aYzsj5uy4l39LxNDpDEB2GGhaYnoVJYHK6OwXbH9W0vnAR4AJ\nSe+cxzGPzDr+EeA4SacCXwTOtf2PuvS0bA71djrket3B9ouSzgM+QInQuZmSMSyiZ3IGEUuSS5KY\npyRtgBIGWtLZdX+d7R22rwYOAqcALwArjuGQrwP+CTwnaQ0ledSMzrofBt4rabWkEUqU0QdmV1bP\ngF5vezvwBeDsY2hbRKOcQcRS9glgTNJVlLy9W4FJ4FuSTqNcU7i3lj0NXFmXo74+3wPZnpT0KPB7\n4C+UZawZNwJ3Sfqr7ffV22fH6/HvtN0Ut38FcLukZfV9V8y3TREvJ7e5RkREoywxRUREoywxRetJ\n+h7w7lnF37X9o0G0J2JYZIkpIiIaZYkpIiIaZYKIiIhGmSAiIqJRJoiIiGiUCSIiIhr9B3h3ru8k\nhB8KAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"27zBR8IspxP8","colab_type":"text"},"source":["From the above plot, we can see that the OOB error rate is low for the classifiers with max_features = 3 in general. Initially the oob error rate is very high, more than 0.4 for each of the three classifiers and with the addition of each new trees, the curves start to fall down. One thing to notice is that, after n_estimators = 18 (approx.), the error rate tends to increase and we can see bumps on each of the curve. Hence, in this case, I would go with max_features = 3 and n_estimator = something in between 15 and 20, may be 17. <br />\n","Let's create a random forest classifier along with these values and see whether it improves the predictive performance compared to what we got in the begining of this tutorial."]},{"cell_type":"code","metadata":{"id":"czXO7pYKktME","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ac99ca4d-9239-482f-d644-65729afa13fe","executionInfo":{"status":"ok","timestamp":1558934191986,"user_tz":-600,"elapsed":751,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["new_model = RandomForestClassifier(n_estimators = 17,\n","                           max_depth = 3,                               \n","                           min_samples_split = 10,                               \n","                           max_features = 3,                               \n","                           random_state = 123\n","                          )\n","new_model.fit(train_x, train_y)\n","\n","new_model_acc = new_model.score(test_x,test_y)\n","\n","print (\"The accuracy of the new model is %f on test dataset\" % new_model_acc)"],"execution_count":109,"outputs":[{"output_type":"stream","text":["The accuracy of the new model is 1.000000 on test dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aoqBMgBNuA-w","colab_type":"text"},"source":["Compared to the accuracy for the model that we created in the begining (which is 0.925926), we have made a nice improvement on the test dataset by simply setting the value for the n_estimators to 17 and max features to 3. In this case we got a test accuracy of 1, which is rare. Here the dataset is tiny compared to the dataset we will be dealing with in real scenario, with fewer input feature space.\n","\n","### Next\n","In the next tutorial, we will be looking at support vector machines. Till then you can try implementing these ensemble techniques on other dataset and compare their predictive performance."]}]}
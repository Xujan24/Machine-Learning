{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Boosting.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IHRKJt5w2H79","colab_type":"text"},"source":["##Ensemble In Machine Learning and Adaptive Boosting (AdaBoost)\n","The dictionary meaning for *ensemble*, according to **English Oxford Dictionary** is as follow: \n","> 1. a group of musicians, actors or dancers perform together.\n",">\n","> 2. a group of items viewed as a whole rather than individually.\n","\n","In machine learning, *ensemble* refers to a group of learning algorithms that combine a definite numbe of weak classifiers or models as a base learner, for instance decision trees, to produce a strong classifier. A weak classifier is a classifier which is able to predict values that are slightly better than the random guesses. And a strong classifer is a classifier that is highly correlated with the true classification, meaining the classifier predicts a correct values most of the time. There are various types of ensemble techniques, of which bagging or bootstrap aggregatting and boosting are the two most popular and commonly used techniques in machine learning community. In this tutorial, we will be looking at boosting ensemble meta-learning algorithm, and look in details, one of the most widely used learning algorithm called `AdaBoost`.\n","\n","### Boosting\n","Boosting is an ensemble technique, in machine learning, which adds multiple base classifiers to produce a strong classifier whose performance can be significantly better than that of any of the base classifiers. When adding the base classifiers, the classifiers are weighted in some ways by their accuracy. And after the weak classifiers are added, the input data are re-weighted such that the input data which are misclassified by the previous weak classifiers gets higher weights and the ones which are correctly classified gets smaller weight. Hence, the subsequent weak classifiers will focus more on the input data that are misclassified by the previous weak classifiers.\n","\n","#### AdaBoost\n","Adaptive boosting, or in short `AdaBoost`, is a boosting algorithm developed by Freund and Schapire in 1996. Since then lots of variants of this algorithm has been formulated and has got its application across a spread of domains.\n","\n","#### Descrete AdaBoost Algorithm\n","Lets consider a two-class classification problem, in which the input data, $ x_1, x_2,...,x_N $ and the corresponding binary target varaibles, $y_1, y_2,...,y_N$ , where $y_n \\in \\{-1,1\\}$. Each data points are given a weight $w_n$, which is initially set to $\\frac{1}{N}$ for all the data points. After training a base learner, $h:x\\rightarrow \\{-1,1\\}$, these weights are adjusted according to the performance of the learner and a new base learner is then trained on these weight-adjusted data points. This process is repeated until the desired number of base classifiers have been trained and finally they are combined to form a committee using cofficients that give different weights to different base classifiers. A more precise form of the Descrete AdaBoost algorithm is given below:\n","\n","1. Initialize the data weighting coefficients {$w_n$} by setting $w_n^{(1)} = \\frac{1}{N}$ for $n=1,2,...,N$.\n","\n","2. For $m=1,2,..., M$:\n","\n","   a. Fit a weak classifier $h_m(x)$ to the training data that minimizes the weighted error function,\\\n","   $J_m=\\sum_{n=1}^N w_n^{(m)}I(h_m(x_n) \\neq y_n)$\\\n","   where $I(h_m(x_n) \\neq y_n)$ is the indicator function whose value is 1 when $y_m(x_n) \\neq  y_n$ and 0 otherwise.\n","\n","    b. Calculate the error rate $\\epsilon_m$ and calculate the weight $\\alpha_m$ as:\\\n","    $\\epsilon_m = \\frac{\\sum_{n=1}^N w_n^{(m)}I(h_m(x_n) \\neq y_n)}{\\sum_{n=1}^N w_n^{(m)}}$ and,\\\n","    $\\alpha_m = \\frac{1}{2}\\ln(\\frac{1-\\epsilon_m}{\\epsilon_m})$\n","\n","   c. Update the data weighting coefficients as:\\\n","   $w_n^{(m+1)} = w_n^{(m)}e^{-y_n\\alpha_m h_m(x_n)}$.\n"," \n","3. Make predictions using the final model, which is given by:\\\n","$h_M(x) = sign(\\sum_{m=1}^M \\alpha_mh_m(x))$ \n","\n","From the above algorithm, we can see that the first base learner, $h_1(x)$, is trained using the initial data weighting coefficients, $w_1$, $2(a)$,  which is set to $\\frac{1}{N}$ for all the data points, *(step 1)*. Then the total error for the learner, $\\epsilon$, is calculated, $2(b)$, which is just the sum of weighting coefficents for all the misclassified data points. And the weight for the learner, $\\alpha$, is calculated using the $\\epsilon$. The weight defines the amount of infulence the learner will have in the final prediction. The weighting coefficients are then updated in such a way that the misclassified data points gets a higher weights and the ones which are correctly identified gets smaller weights, as described on step $2.c$. This process is repeated until the desired number of learners, i.e. $M$, are trained and finally the prediction can be made using the ensemble of these base learners, *(step 3)*.<br /><br />\n","To learn more about boosting and `AdaBoost`, the following are some important resources:\n","1. [Thoughts on Hypothesis Boosting](http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf), Kearns (1988).\n","2. [A decision-theoretic generalization of on-line learning and an application to boosting](https://www.cis.upenn.edu/~mkearns/teaching/COLT/adaboost.pdf), Freund and Schapire (1996), (Section 4: Boosting).\n","1. [Additive Logestic Regression: A statistical view of Boosting](https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf), Friedman *et al.*(2000)\n","2. [AdaBoost and the Super Bowl of ClassifiersA Tutorial Introduction to Adaptive Boosting](http://www.inf.fu-berlin.de/inst/ag-ki/adaboost4.pdf), Rojas (2009).\n","3. [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost), Wikipedia.\n","4. [Video Tutorial](https://www.youtube.com/watch?v=LsK-xG1cLYA&t=883s), StatQuest with Josh Starmer.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QcoG-ZG8plPi","colab_type":"text"},"source":["#### AdaBoost in Action\n","Now, we have some fundamental knowledge of the theory behind boosting and `AdaBoost`, we will implement the `AdaBoost` algorithm using `scikit-learn`. In this tutorial, we will be using the wine dataset. The dataset has 13 real valued input features and three output classes."]},{"cell_type":"code","metadata":{"id":"-Hx6anX61_6H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"c8d870ff-01dc-4cfd-adf1-c9c0528e8c9b","executionInfo":{"status":"ok","timestamp":1557463203003,"user_tz":-600,"elapsed":848,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["from sklearn import datasets\n","wineData = datasets.load_wine()        #returns a bunch object\n","\n","# get the input features X and corresponding classes Y\n","X, Y = wineData.data, wineData.target\n","\n","# get the labels for each class\n","classLabels = wineData.target_names\n","\n","# print some information about the dataset\n","num_samples, num_features = X.shape\n","print(\"The input dataset contains %d samples and %d input features\" % (num_samples, num_features))\n","print(\"The class labels are: %s\" % classLabels)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["The input dataset contains 178 samples and 13 input features\n","The class labels are: ['class_0' 'class_1' 'class_2']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FelVRcuqt_FK","colab_type":"code","colab":{}},"source":["# As usual, we will split the dataset into train and test dataset.\n","from sklearn.model_selection import train_test_split\n","\n","train_x, test_x, train_y, test_y = train_test_split(X, Y,\n","                                                    test_size=0.3,    # 30% will be used as test set\n","                                                    random_state=123, # random seed\n","                                                    shuffle=True      # shuffle the dataset when splitting\n","                                                   )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nA_45wtVwidX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b8265ddf-fa39-413b-dad4-e4513b69eedd","executionInfo":{"status":"ok","timestamp":1557463220848,"user_tz":-600,"elapsed":1100,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["# create an AdaBoost classifier\n","# we can set the number of weak learners through the parameter n_estimator\n","# and the learning algorithm, either discrete or real, throught the parameter algorithm\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","clf = AdaBoostClassifier(n_estimators=3,\n","                         algorithm = \"SAMME.R\" # real adaboost algorithm, converses faster\n","                        )\n","\n","# train the classifier using the training dataset.\n","clf.fit(train_x, train_y)\n","\n","# calculate the accuracy of the trained model on test dataset\n","acc = clf.score(test_x, test_y)\n","\n","print(\"The accuracy of our model is %f on test dataset.\" % acc)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["The accuracy of our model is 0.648148 on test dataset.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6FJmqjdOzNko","colab_type":"text"},"source":["####Determining the number of weak learners (Hyperparameter Tuning)\n","There is nothing such as rule of thumb for choosing the number of weak learners. You can choose any numbers of weak learners and a specific number of weak learners can yeld better predictive performance than other. So, the question is how do we choose the number of weak learners?\\\n","These kind of parameters, which are to be set manually before the actual learning begins, are called hyper-parameters. And there are various techniques for tuning these kind of hyper-parameters. One traditional way of doing this is *via* grid search, which simply search for the optimal value through a manually specified subset of values."]},{"cell_type":"code","metadata":{"id":"CEVuMtj02WQe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"787f2912-8e26-4f13-9956-9449ed40a1f7","executionInfo":{"status":"ok","timestamp":1557463227186,"user_tz":-600,"elapsed":731,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["from sklearn.model_selection import GridSearchCV\n","\n","# specify a set of values for the hyperparameter to tune\n","# in this case we want to tune n_estimators\n","\n","parameters = {\n","    'n_estimators': [2,4,6,8]\n","}\n","\n","# create a new adaboost classifier\n","model = AdaBoostClassifier()\n","\n","# create grid search\n","grid = GridSearchCV(model, parameters)\n","\n","# perform grid search\n","grid.fit(train_x, train_y)\n"],"execution_count":55,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n","  warnings.warn(CV_WARNING, FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv='warn', error_score='raise-deprecating',\n","       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n","          learning_rate=1.0, n_estimators=50, random_state=None),\n","       fit_params=None, iid='warn', n_jobs=None,\n","       param_grid={'n_estimators': [2, 4, 6, 8]}, pre_dispatch='2*n_jobs',\n","       refit=True, return_train_score='warn', scoring=None, verbose=0)"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"QBKFB-aG3e_7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"12bb8b77-57d1-4782-9156-b97b4bdb1eac","executionInfo":{"status":"ok","timestamp":1557463231588,"user_tz":-600,"elapsed":649,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["# <GridSearchCV>.best_estimator_ gives you the best classifier\n","print(\"The best Classifier is:\")\n","print(grid.best_estimator_)\n","\n","# <GridSearchCV>.best_params_ gives the best values for the hyper parameters\n","print(\"\\n The best number of weak learners is:\")\n","print(grid.best_params_)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["The best Classifier is:\n","AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n","          learning_rate=1.0, n_estimators=6, random_state=None)\n","\n"," The best number of weak learners is:\n","{'n_estimators': 6}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"68w3pc4U4XqS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"249b880c-5846-4608-c9d3-b639c56bf2b6","executionInfo":{"status":"ok","timestamp":1557463238250,"user_tz":-600,"elapsed":818,"user":{"displayName":"Santosh Purja Pun","photoUrl":"https://lh3.googleusercontent.com/-lSFl4QnIVIk/AAAAAAAAAAI/AAAAAAAAAIY/zqCf6YZz0-4/s64/photo.jpg","userId":"11631572469968879053"}}},"source":["# Now lets calculate the test score for the best classifier\n","best_clf = grid.best_estimator_\n","acc_best_clf = best_clf.score(test_x, test_y)\n","\n","print(\"The test accuracy for the best classifier is: %f\" % acc_best_clf)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["The test accuracy for the best classifier is: 0.870370\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aCFgWT0f6YF-","colab_type":"text"},"source":["Hence, by fine tuning the hyper-parameter, i.e. the number of weak learners in this case, we were able to improve the accuracy of the model on the test set. In this tutorial, I set the number of weak learners to a small number, because our dataset is small. However, in real case scenario, people often set the number of weak learners to 100 because the size of the dataset you will be dealing with would be hundreds or even thousands times larger than our toy dataset. Having said that, it doesn't necessarily mean that you should always use 100 as the number of learners. Always perform some kind of hyper-parameter tuning before coming to a specific value. One cool thing about boosting and `AdaBoost` is that they are very robost to overfitting.\n","\n","## Next\n","There are many varients of boosting algorithm, for instance, XGBoost, LogitBoost, Gradient Boosted Trees, etc. The theory behind all of these algorithms are similar to `AdaBoost` algorithm. So, you can go through some of these boosting algorithms, you can easily find them on the internet and the `scikit-learn` package has support for a lot of these ensemble techniques,  you can learn them from their official website. In the next tutorial, we will look at **bootstrap aggregating**, aslo known as ***bagging***, and **Random Forest**."]}]}